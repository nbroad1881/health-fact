# RunArguments
DEBUG: no
n_gpu: 1
k_folds: 5
num_proc: 7
max_seq_length: 2048
pad_multiple: 1024
padding: no
stride: 0
model_name_or_path: "google/bigbird-roberta-base"
reinit_layers: 2
layer_norm_eps: 1e-7
output_dropout: 0.0
use_8bit: no
use_layerwise_lr: no
layer_lr_alpha: 0.8
use_lookahead: no
use_sift: no

# bigbird
block_size: 128
num_random_blocks: 6

# swa
use_swa: no
swa_start_after: 6800
swa_save_every: 25
swa_lr: 1e-7

# WandB Config
project: health-fact
entity: nbroad
group: none
tags:
  - bigbird-base
notes: >
 
job_type: train

# TrainingArguments
# <default> means use default value
training_arguments:
# output
  output_dir: "./out"
  overwrite_output_dir: yes

# dataloader
  dataloader_num_workers: 2
  dataloader_pin_memory: yes

# training
  do_train: yes
  resume_from_checkpoint:
  seed: 18

# hyperparams
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  gradient_checkpointing: no
  group_by_length: yes
  learning_rate: 5e-6
  weight_decay: .01

# schedule + steps
  num_train_epochs: 1
  lr_scheduler_type: linear
  warmup_ratio: 0.1
  warmup_steps: 0
  max_steps: -1

# optimizer
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-6
  max_grad_norm: 1.0
  optim: 'adamw_hf'
  adafactor: no

# logging
  log_level: "warning"
  log_level_replica: <default>
  log_on_each_node: <default>
  logging_dir: <default>
  logging_strategy: steps
  logging_first_step: no
  logging_steps: 100
  logging_nan_inf_filter: <default>

# dtype
  bf16: yes
  fp16: no
  fp16_opt_level: "O1"
  half_precision_backend: "auto"
  bf16_full_eval: no
  fp16_full_eval: no
  tf32: no

# saving 
  save_strategy: "no"
  save_steps: 300000
  save_total_limit: 1
  load_best_model_at_end: no
  metric_for_best_model: "eval_f1"

# evaluation
  do_eval: yes
  evaluation_strategy: "epoch"
  eval_steps: 20
  eval_delay: 0
  per_device_eval_batch_size: 32
  do_predict: no
  
# hub
  push_to_hub: no
  hub_private_repo: yes
  hub_model_id: <default>
  hub_strategy: "every_save"
  hub_token: <default>

# misc
  report_to: "wandb"

# rarely used
  debug: <default>
  prediction_loss_only: <default>
  per_gpu_train_batch_size: <default>
  per_gpu_eval_batch_size: <default>
  eval_accumulation_steps: <default>
  save_on_each_node: <default>
  no_cuda: <default>
  local_rank: <default>
  xpu_backend: <default>
  tpu_num_cores: <default>
  tpu_metrics_debug: <default>
  dataloader_drop_last: <default>
  past_index: <default>
  run_name: <default>
  disable_tqdm: <default>
  remove_unused_columns: <default>
  label_names: <default>
  greater_is_better: <default>
  ignore_data_skip: <default>
  sharded_ddp: <default>
  deepspeed: <default>
  label_smoothing_factor: <default>
  length_column_name: <default>
  ddp_find_unused_parameters: <default>
  ddp_bucket_cap_mb: <default>
  skip_memory_metrics: <default>
  use_legacy_prediction_loop: <default>
  fp16_backend: <default>
  mp_parameters: <default>